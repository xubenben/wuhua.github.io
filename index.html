<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html dir="LTR" lang="en">

<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>Hua Wu</title>
<script type="text/javascript"></script>
<link rel="stylesheet" media="all" type="text/css"
      href="style.css"> 
<meta name="description" content="Hua Wu">
<!--[if lte IE 6]><link rel="stylesheet" media="screen" type="text/css" href="ie6.css"><![endif]-->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-3578529-3");
pageTracker._trackPageview();
} catch(err) {}</script>
<style>
sub, sup {
  /* Specified in % so that the sup/sup is the right size relative to the surrounding text */
  font-size: 75%;

  /* Zero out the line-height so that it doesn't interfere with the positioning that follows */
  line-height: 0;

  /* Where the magic happens: makes all browsers position the sup/sup properly, relative to the surrounding text */
  position: relative;

  /* Note that if you're using Eric Meyer's reset.css, this is already set and you can remove this rule */
  vertical-align: baseline;
}

sup {
  /* Move the superscripted text up */
  top: -0.5em;
}

sub {
  /* Move the subscripted text down, but only half as far down as the superscript moved up */
  bottom: -0.25em;
}
</style>
</head>

<body>

<div id="wrapper">$
    <ol id="nav">
    
    <li><a class="selected" href="./index.html" title="Home">Home</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#act" title="Activities">Activities</a></li>
    <li><a href="#res" title="Research">Research</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
	<!-- <li><a href="#pat" title="Patents">Patents</a></li> -->
    <!-- <li><a href="./files/cv.pdf" title="Resume">Resume</a></li> -->
	<!-- <li><a href="#links" title="Friends">Friends</a></li> -->
    </ol>

<!-- <div id="wrapper"> -->

<h1 id="fff"><a href="http://xubenben.github.io" title="Hua Wu">
	Hua Wu </a></h1>

<div id="content">
<img id="head_photo" src="./images/wuhua.jpg" style="width: 20%; height: 20%" align="left"/>
<!-- <p><em>Welcome!</em></p> -->

<br/>
<em>Chief Scientist</em> of <a href="https://nlp.baidu.com/homepage/index">Baidu NLP</a><br/>
<em>Chair</em> of Baidu Technical Committee <br/>
<em>Email</em>: wu_hua@baidu.com<br/>
<em>Address</em>: Baidu Technology Park Building No. 1, No. 10 Xibeiwang East Road, Haidian District, Beijing, 100093, China
<br/>
	
<div id="clear"></div>
	
<!--<h1>Biography and Interest</h1>-->	
<br/>
I joined <a href="https://ir.baidu.com/">Baidu</a> in 2010. Now I am the technical leader of Baidu NLP department and knowledge graph department. Before that, I worked for Toshiba (China) R&D Center and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> (MSRA). I obtained Ph.D. degree in pattern recognition and intelligent system from the <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Science</a> in 2001.
<br/><br/>
	
My research interest includes dialogue systems, machine translation, natural language processing and knowledge graph.
<!-- <div id="person_intro">



</div> -->



<!-- <h1>News</h1>
<ul>
	<li>
		<img valign="top" src="images/new.gif" width="40" border="0">
		I have successfully defended my Ph.D. dissertation and I will join Microsoft Research Asia in September, 2014. 
		<div id="clear"></div>
	</li>
</ul> -->

<a name="news"></a>
<div id="news">
<h1>News</h1>
<ul>
    <li>
        <font color="red">We are hiring (both interns and employees)!</font> <em>Please drop me an email with your resume</em> if you are interested in working with us on NLP problems, including but not limited to Dialogue Systems, Machine Translation, Question Answering, Distributed Representation, Generation, Knowledge Graph. Experiences with machine (incl. but not limited to deep) learning for NLP are preferred.
    </li>
    <li>
        We are organizing the Workshop on Simultaneous Translation (<a href="https://autosimtrans.github.io/">2021</a>, <a href="https://autosimtrans.github.io/2020/">2020</a>), where there is a shared task on Chinese-English and English-Spanish simultaneous translation. 
    </li>
    <li>
        Our <a href="https://arxiv.org/pdf/2006.16779.pdf">PLATO-2</a> model was ranked top 1 at DSTC9 tracks 1, 2 and 3.
    </li>
    <li>
        We launched <a href="https://www.luge.ai/">LUGE</a> (Language Understanding and Generation Evaluation Benchmarks ) on Chinese NLP, which aims to provide researchers with various kinds of data sets and evaluations, and jointly promote the progress of Chinese NLP technology. A recent introduction on this is available <a href="https://www.bilibili.com/video/BV1jk4y1y7Y6?from=search&seid=17592121354154345012">here</a> (In Chinese). If you are interested in LUGE or sharing data sets, pls. contact me.
    </li>
</ul>
</div>

<a name="act"></a>
<!-- <div id="line"></div> -->

<div id="Activities">
    <h1>Professional Activities</h1>

    <ul>
    <li>Program co-chair of AACL 2020, ACL 2014</li>
    <li>Area chairs of or SPC of ACL, IJCAI and AAAI</li>
    <li>Co-organize the first Workshop on <a href="https://autosimtrans.github.io/2020/">Automatic Simultaneous Translation 2020</a></li>
    <li>Co-organize the ICDAR Workshop of Document Image and Language 2021</li>
    </ul>
    <div id="clear"></div>
</div>

<!-- <h1>Research Interests</h1>
<ul>
    <li> Dialogue Systems, Machine Translation, Natural Language Processing, Knowledge Graph</li>
</ul> -->


<a name="res"></a>
<div id="Research">
<h1>Research</h1>
    <h2>&spades; <em>Open-Domain Dialogue Systems</em></h2>
    The aim of the open domain dialogue system is let the machines capable of chatting, answering question and completing tasks, as well as the ability of rapid learning and continuous evolution. Its core competencies are as follows:
    <ul>
        <li>Understanding: understand natural languages</li>
        <li>Expression: express in fluent natural languages</li>
        <li>Emotion: understand emotions and respond with appropriate emotions</li>
        <li>Thinking: Context-based calculation, reasoning and decision making</li>
        <li>Learning: Capable of learning and evolution</li>
    </ul>
    It is not easy to make such a system come true. There are several fundamental problems to be solved: dialogue-oriented knowledge representation, knowledge-grounded policy learning, knowledge-grounded response generation. In order to approach this target, we have conducted some researches:<br>
    <ul>
        <li>
            <em>Large-scale pre-trained response generation model</em>:  <br>
            Based on the available large-scale open-domain conversation, we pre-trained a response generation model <a href="https://arxiv.org/pdf/2006.16779.pdf">PLATO-2</a> via curriculum learning. We have released our English models and source codes at <a href="https://github.com/PaddlePaddle/Knover/tree/master/plato-2">Github</a>. PLATO-2 was ranked top 1 at <a href="https://dstc9.dstc.community/tracks">DSTC 9</a> Track 1, Track2, and Track 3 shared tasks.
        </li>
        <li>
            <em>Knowledge-grounded policy learning and response generation</em>: <br>
            we leverage graphs to guide policy learning. Different kinds of graphs are used including knowledge graphs, conversation graphs constructed from query logs, event graphs constructed from stories. Several papers were published in <a href="http://ir.hit.edu.cn/~car/papers/AAAI2020-Xu-kgconv.pdf">AAAI 2020</a>, <a href="https://www.aclweb.org/anthology/2020.acl-main.166.pdf">ACL 2020</a>, <a href="https://www.ijcai.org/Proceedings/2020/0545.pdf">IJCAI 2020</a>.
        </li>
        <li>
            <em>Datasets for knowledge-grounded dialogue system</em><br>
            <a href="https://arxiv.org/pdf/1906.05572.pdf">DuCov</a>: This corpus is designed to facilitate the researches towards building a human-like conversational agent: endowing it with the ability of proactively leading the conversation. In DuConv, one acts as a conversation leader and the other acts as the follower. The leader is provided with a knowledge graph and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. DuConv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph. This dataset contains about 270K utterances and 30k dialogues.<br>
            <br><a href="https://www.aclweb.org/anthology/2020.acl-main.98.pdf">DuRecDial</a>: This corpus is designed to facilitate conversational recommendation over multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, considering user’s interests and feedback. DuRecDial contains about 10k dialogs, 156k utterances. In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. <br>
        </li>
    </ul>

    <h2>&spades; <em>Machine Translation</em></h2>
    Since 2010, we have been working on an online machine translation product named <a href="https://fanyi.baidu.com/">Baidu Translate</a>, which translates among 203 languages. In 2011, we launched the statistical machine translation service. In May, 2015, we launched the world’s first neural machine translation service. Besides text translation, Baidu Translate supports speech-to-speech translation, simultaneous translation, and OCR/image translation.
    <ul>
    <li>
        <em>Simultaneous Translation</em><br>
        We co-organized the first Workshop on <a href="https://autosimtrans.github.io/2020/">Automatic Simultaneous Translation 2020</a>, where we release the first Chinese-English simultaneous translation <a href="https://autosimtrans.github.io/2020/shared.html#dataset">dataset</a>, which contains about 70 hours of Chinese speech audio, human transcripts, ASR results and English translations. In order to make tradeoff between translation quality and translation efficiency, we proposed several methods including <a href="https://www.aclweb.org/anthology/P19-1289.pdf">wait-k</a> and <a href="https://www.aclweb.org/anthology/2020.emnlp-main.178.pdf">adaptive meaningful units segmentation method</a>.
    </li>
    <li>
        <em>Multilingual Translation</em><br>
        For most of language pairs such as Chinese-Spanish, Chinese-Japanese, Chinese-Thai Language, there exists data sparseness problems. Besides pivot language approaches, we proposed the <a href="https://www.aclweb.org/anthology/P15-1166.pdf">one to many translation method</a> in 2015, which shares the source language encode, and use individual decodes for each target language.
    </li>
    </ul>

    <h2>&spades; <em>Natural Language Processing</em></h2>
    <ul>
    <li>
        <em>Pre-trained Model ERNIE</em><br>
        Recently pre-trained models have achieved state-of-the-art results in various language understanding tasks. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named <a href="https://arxiv.org/pdf/1907.12412.pdf">ERNIE 2.0</a> which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The <a href="https://github.com/PaddlePaddle/ERNIE">source codes and pre-trained models</a> have been released.
    </li>
    <li>
        <em>Question Answering and Machine Reading Comprehension</em><br>
        We developed Question Answering and Machine Reading Comprehension methods, which are used in <a href="https://www.baidu.com/">Baidu search engine</a>. Recently, we proposed <a href="https://arxiv.org/abs/2010.08191">RocketQA</a>, an optimized training approach to dense passage retrieval for open-domain question answering. RocketQA achieved the 1st rank at the leaderboard of MSMARCO Passage Ranking Task. We released a Chinese dataset namely <a href="https://arxiv.org/pdf/2004.11142">DuReader<SUB>robust</SUB></a> towards evaluating the robustness of machine reading comprehension models, and we hosted a shared task based on DuReader<SUB>robust</SUB> [<a href="https://github.com/PaddlePaddle/Research/tree/master/NLP/DuReader-Robust-BASELINE">Data&Code</a>, <a href="https://aistudio.baidu.com/aistudio/competition/detail/28?isFromCcf=true">Leaderboard</a>].
    </li>
    </ul>


</div>

<a name="pub"></a>
<div id="publication">

<!-- Publications -->
<h1>Papers [<a href="https://scholar.google.com/citations?user=9X2ThuAAAAAJ&hl=en">Google Scholar</a>]</h1>

<ul>
    <!-- <h2>Conferences & Workshops</h2> -->
<br>

    <li>
        <em><a href="https://arxiv.org/pdf/2006.16934.pdf">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</a></em>
        <br>Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://arxiv.org/pdf/2006.16779.pdf">Plato-2: Towards building an open-domain chatbot via curriculum learning</a></em>
        <br>Siqi Bao, Huang He, Fan Wang, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, Xinchao Xu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="publication/xu-knowledge-2020/xu-knowledge-2020.pdf">Knowledge Graph Grounded Goal Planning for Open-Domain Conversation Generation</a></em>, AAAI 2020
        <br>Jun Xu, Haifeng Wang, Zhengyu Niu, <strong><u>Hua Wu</u></strong>, Wanxiang Che<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://arxiv.org/pdf/1907.12412.pdf?source=post_page">ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding.</a></em>, AAAI 2020
        <br>Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://arxiv.org/pdf/1912.07240.pdf">Synchronous Speech Recognition and Speech-to-Text Translation with Interactive Decoding</a></em>, AAAI 2020
        <br>JYuchen Liu, Jiajun Zhang, Hao Xiong, Long Zhou, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Chengqing Zong<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.acl-main.98/">Towards Conversational Recommendation over Multi-Type Dialogs</a></em>, ACL 2020
        <br>Zeming Liu, Haifeng Wang, Zheng-Yu Niu, <strong><u>Hua Wu</u></strong>, Wanxiang Che, Ting Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.acl-main.166/">Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation</a></em>, ACL 2020
        <br>Jun Xu, Haifeng Wang, Zheng-Yu Niu, <strong><u>Hua Wu</u></strong>, Wanxiang Che, Ting Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</a></em>, ACL 2020
        <br>Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Feng Wu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.acl-main.555/">Leveraging Graph to Improve Abstractive Multi-Document Summarization</a></em>, ACL 2020
        <br>Wei Li, Xinyan Xiao, Jiachen Liu, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Junping Du<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.acl-main.639/">Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer</a></em>, ACL 2020
        <br>Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable</a></em>, ACL 2020
        <br>Siqi Bao, Huang He, Fan Wang, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.69/">Syntactic and Semantic-driven Learning for Open Information Extraction</a></em>, EMNLP (Findings) 2020
        <br>Jialong Tang, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Xinyan Xiao, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.emnlp-main.178/">Learning Adaptive Segmentation Policy for Simultaneous Translation</a></em>, EMNLP (1) 2020
        <br>Ruiqing Zhang, Chuanqiang Zhang, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.emnlp-main.562.pdf">ChiTeSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset</a></em>, EMNLP 2020
        <br>Lijie Wang, Ao Zhang, Kun Wu, Ke Sun, Zhenghua Li, <strong><u>Hua Wu</u></strong>, Min Zhang, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset</a></em>, EMNLP (1) 2020
        <br>Lijie Wang, Ao Zhang, Kun Wu, Ke Sun, Zhenghua Li, <strong><u>Hua Wu</u></strong>, Min Zhang, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/2020.emnlp-main.570/">Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification</a></em>, EMNLP (1) 2020
        <br>Yunjie Ji, Hao Liu, Bolei He, Xinyan Xiao, <strong><u>Hua Wu</u></strong>, Yanhua Yu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.24963/ijcai.2020/545">Enhancing Dialog Coherence with Event Graph Grounded Content Planning</a></em>, IJCAI 2020
        <br>Jun Xu, Zeyang Lei, Haifeng Wang, Zheng-Yu Niu, <strong><u>Hua Wu</u></strong>, Wanxiang Che<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation</a></em>, IJCAI 2020
        <br>Dongling Xiao, Han Zhang, Yu-Kun Li, Yu Sun, Hao Tian, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>DuReader<SUB>robust</SUB>: A Chinese Dataset Towards Evaluating the Robustness of Machine Reading Comprehension Models</a></em>, CoRR abs/2004
        <br>Hongxuan Tang, Jing Liu, Hongyu Li, Yu Hong, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="http://arxiv.org/abs/1811.05683">Modeling Coherence for Discourse Neural Machine Translation</a></em>, AAAI 2019
        <br>Hao Xiong, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aaai.org/ojs/index.php/AAAI/article/download/3817/3695">Addressing the Under-Translation Problem from the Entropy Perspective</a></em>, AAAI 2019
        <br>Yang Zhao, Jiajun Zhang, Chengqing Zong, Zhongjun He, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification</a></em>, ACL (1) 2019
        <br>Wei Jia, Dai Dai, Xinyan Xiao, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Know More about Each Other: Evolving Dialogue Strategy via Compound Assessment</a></em>, ACL (1) 2019
        <br>Siqi Bao, Huang He, Fan Wang, Rongzhong Lian, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Proactive Human-Machine Conversation with Explicit Conversation Goal</a></em>, ACL (1) 2019
        <br>Wenquan Wu, Zhen Guo, Xiangyang Zhou, <strong><u>Hua Wu</u></strong>, Xiyuan Zhang, Rongzhong Lian, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/p19-1226">Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a></em>, ACL (1) 2019
        <br>An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, <strong><u>Hua Wu</u></strong>, Qiaoqiao She, Sujian Li<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework</a></em>, ACL (1) 2019
        <br>Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>D-NET: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension</a></em>, MRQA@EMNLP 2019
        <br>Hongyu Li, Xiyuan Zhang, Yibing Liu, Yiming Zhang, Quan Wang, Xiangyang Zhou, Jing Liu, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/D19-1047">Enhancing Local Feature Extraction with Global Representation for Neural Text Classification</a></em>, EMNLP/IJCNLP (1) 2019
        <br>Guocheng Niu, Hengru Xu, Bolei He, Xinyan Xiao, <strong><u>Hua Wu</u></strong>, Sheng Gao<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/D19-1079">Multi-agent Learning for Neural Machine Translation</a></em>, EMNLP/IJCNLP (1) 2019
        <br>Tianchi Bi, Hao Xiong, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/D19-1187">Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</a></em>, EMNLP/IJCNLP (1) 2019
        <br>Zhibin Liu, Zheng-Yu Niu, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/978-3-030-32236-6_6">A Key-Phrase Aware End2end Neural Response Generation Model</a></em>, NLPCC (2) 2019
        <br>Jun Xu, Haifeng Wang, Zhengyu Niu, <strong><u>Hua Wu</u></strong>, Wanxiang Che<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/978-3-030-32236-6_75">An Overview of the 2019 Language and Intelligence Challenge</a></em>, NLPCC (2) 2019
        <br>Quan Wang, Wenquan Wu, Yabing Shi, Hongyu Li, Zhen Guo, Wei He, Hongyu Liu, Ying Chen, Yajuan Lyu, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Baidu Neural Machine Translation Systems for WMT19</a></em>, WMT (2) 2019
        <br>Meng Sun, Bojian Jiang, Hao Xiong, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Learning to Select Knowledge for Response Generation in Dialog Systems</a></em>, IJCAI 2019
        <br>Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Generating Multiple Diverse Responses with Multi-Mapping and Posterior Mapping Selection</a></em>, IJCAI 2019
        <br>Chaotao Chen, Jinhua Peng, Fan Wang, Jun Xu, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>ERNIE: Enhanced Representation through Knowledge Integration</a></em>, CoRR abs/1904
        <br>Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting</a></em>, CoRR abs/1907
        <br>Hao Xiong, Ruiqing Zhang, Chuanqiang Zhang, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>CoKE: Contextualized Knowledge Graph Embedding</a></em>, CoRR abs/1911
        <br>Quan Wang, Pingping Huang, Haifeng Wang, Songtai Dai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong Zhu, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.researchgate.net/profile/Di_Jiang12/publication/333076009_Integrating_Bayesian_and_Neural_Networks_for_Discourse_Coherence/links/5ce639ff299bf14d95b1e389/Integrating-Bayesian-and-Neural-Networks-for-Discourse-Coherence.pdf">Companion Proceedings of The 2019 World Wide Web Conference</a></em>
        <br>Jinhua Peng, Zongyang Ma, Di Jiang, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://arxiv.org/pdf/1904.08075.pdf">End-to-end speech translation with knowledge distillation</a></em>
        <br>Yuchen Liu, Hao Xiong, Zhongjun He, Jiajun Zhang, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Chengqing Zong<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16788">Multi-Channel Encoder for Neural Machine Translation</a></em>, AAAI 2018
        <br>Hao Xiong, Zhongjun He, Xiaoguang Hu, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</a></em>, QA@ACL 2018
        <br>Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, <strong><u>Hua Wu</u></strong>, Qiaoqiao She, Xuan Liu, Tian Wu, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P18-1103/">Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</a></em>, ACL (1) 2018
        <br>Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P18-1178/">Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification</a></em>, ACL (1) 2018
        <br>Yizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu, <strong><u>Hua Wu</u></strong>, Sujian Li, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/d18-1036">Addressing Troublesome Words in Neural Machine Translation</a></em>, EMNLP 2018
        <br>Yang Zhao, Jiajun Zhang, Zhongjun He, Chengqing Zong, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://openreview.net/forum?id=BkSDMA36Z">A New Method of Region Embedding for Text Classification</a></em>, ICLR (Poster) 2018
        <br>Chao Qiao, Bo Huang, Guocheng Niu, Daren Li, Daxiang Dong, Wei He, Dianhai Yu, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://arxiv.org/pdf/1808.03733.pdf">Familia: A configurable topic modeling framework for industrial text engineering</a></em>
        <br>Di Jiang, Yuanfeng Song, Rongzhong Lian, Siqi Bao, Jinhua Peng, Huang He, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/P17-1021">An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge</a></em>, ACL (1) 2017
        <br>Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi Liu, <strong><u>Hua Wu</u></strong>, Jun Zhao<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Improved Neural Machine Translation with SMT Features</a></em>, AAAI 2016
        <br>Wei He, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/p16-1185">Semi-Supervised Learning for Neural Machine Translation</a></em>, ACL (1) 2016
        <br>Yong Cheng, Wei Xu, Zhongjun He, Wei He, <strong><u>Hua Wu</u></strong>, Maosong Sun, Yang Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/p16-1033">Active Learning for Dependency Parsing with Partial Annotation</a></em>, ACL (1) 2016
        <br>Zhenghua Li, Min Zhang, Yue Zhang, Zhanyi Liu, Wenliang Chen, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/p16-1159">Minimum Risk Training for Neural Machine Translation</a></em>, ACL (1) 2016
        <br>Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, <strong><u>Hua Wu</u></strong>, Maosong Sun, Yang Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1145/2983323.2983360">"Shall I Be Your Chat Companion?": Towards an Online Human-Computer Conversation System</a></em>, CIKM 2016
        <br>Rui Yan, Yiping Song, Xiangyang Zhou, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/C16-1100/">Chinese Poetry Generation with Planning based Neural Network</a></em>, COLING 2016
        <br>Zhe Wang, Wei He, <strong><u>Hua Wu</u></strong>, Haiyang Wu, Wei Li, Haifeng Wang, Enhong Chen<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/C16-1253.pdf">Latent topic embedding</a></em>, COLING 2016
        <br>Di Jiang, Lei Shi, Rongzhong Lian, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.18653/v1/d16-1036">Multi-view Response Selection for Human-Computer Conversation</a></em>, EMNLP 2016
        <br>Xiangyang Zhou, Daxiang Dong, <strong><u>Hua Wu</u></strong>, Shiqi Zhao, Dianhai Yu, Hao Tian, Xuan Liu, Rui Yan<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="http://www.ijcai.org/Abstract/16/392">Agreement-Based Joint Training for Bidirectional Attention-Based Neural Machine Translation</a></em>, IJCAI 2016
        <br>Yong Cheng, Shiqi Shen, Zhongjun He, Wei He, <strong><u>Hua Wu</u></strong>, Maosong Sun, Yang Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1145/2911451.2911542">Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System</a></em>, SIGIR 2016
        <br>Rui Yan, Yiping Song, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="http://arxiv.org/abs/1606.00979">Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information</a></em>, CoRR abs/1606
        <br>Yuanzhe Zhang, Kang Liu, Shizhu He, Guoliang Ji, Zhanyi Liu, <strong><u>Hua Wu</u></strong>, Jun Zhao<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.3115/v1/p15-1166">Multi-Task Learning for Multiple Language Translation</a></em>, ACL (1) 2015
        <br>Daxiang Dong, <strong><u>Hua Wu</u></strong>, Wei He, Dianhai Yu, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1145/2736277.2741107">Exploiting Collective Hidden Structures in Webpage Titles for Open Domain Entity Extraction</a></em>, WWW 2015
        <br>Wei Song, Shiqi Zhao, Chao Zhang, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Lizhen Liu, Hanshi Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.3115/v1/d14-1007">Policy Learning for Domain Selection in an Extensible Multi-domain Spoken Dialogue System</a></em>, EMNLP 2014
        <br>Zhuoran Wang, Hongliang Chen, Guanchun Wang, Hao Tian, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.3115/v1/d14-1015">Improve Statistical Machine Translation with Context-Sensitive Bilingual Semantic Embedding Model</a></em>, EMNLP 2014
        <br>Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dianhai Yu, Wei He, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Ting Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.3115/v1/d14-1016">Transformation from Discontinuous to Continuous Word Alignment Improves Translation Quality</a></em>, EMNLP 2014
        <br>Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Ting Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.3115/v1/d14-1174">Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs</a></em>, EMNLP 2014
        <br>Xiaoning Zhu, Zhongjun He, <strong><u>Hua Wu</u></strong>, Conghui Zhu, Haifeng Wang, Tiejun Zhao<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/D13-1050/">Improving Pivot-Based Statistical Machine Translation Using Random Walk</a></em>, EMNLP 2013
        <br>Xiaoning Zhu, Zhongjun He, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Conghui Zhu, Tiejun Zhao<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/W13-5708.pdf">Generalization of words for chinese dependency parsing</a></em>, IWPT 2013
        <br>Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dianhai Yu, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P12-1048/">Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></em>, ACL (1) 2012
        <br>Jinsong Su, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong, Qun Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></em>, ACL (1) 2012
        <br>Wei He, <strong><u>Hua Wu</u></strong>, Haifeng Wang, Ting Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/978-3-642-35341-3_42">Opening Machine Translation Black Box for Cross-Language Information Retrieval</a></em>, AIRS 2012
        <br>Yanjun Ma, Jian-Yun Nie, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1145/2036264.2036280">Two-Word Collocation Extraction Using Monolingual Word Alignment Method</a></em>, ACM Trans
        <br>Zhanyi Liu, Haifeng Wang, <strong><u>Hua Wu</u></strong>, Sheng Li<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P11-1104/">Reordering with Source Language Collocations</a></em>, ACL 2011
        <br>Zhanyi Liu, Haifeng Wang, <strong><u>Hua Wu</u></strong>, Ting Liu, Sheng Li<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P10-1085/">Improving Statistical Machine Translation with Monolingual Collocation</a></em>, ACL 2010
        <br>Zhanyi Liu, Haifeng Wang, <strong><u>Hua Wu</u></strong>, Sheng Li<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P09-1006/">Exploiting Heterogeneous Treebanks for Parsing</a></em>, ACL/IJCNLP 2009
        <br>Zheng-Yu Niu, Haifeng Wang, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P09-1018/">Revisiting Pivot Language Approach for Machine Translation</a></em>, ACL/IJCNLP 2009
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/D09-1051/">Collocation Extraction Using Monolingual Word Alignment Method</a></em>, EMNLP 2009
        <br>Zhanyi Liu, Haifeng Wang, <strong><u>Hua Wu</u></strong>, Sheng Li<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/C08-1125/">Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora</a></em>, COLING 2008
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang, Chengqing Zong<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Predicting and Tagging Dialog-Act Using MDP and SVM</a></em>, ISCSLP 2008
        <br>Keyan Zhou, Chengqing Zong, <strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>The TCH machine translation system for IWSLT 2008</a></em>, IWSLT 2008
        <br>Haifeng Wang, <strong><u>Hua Wu</u></strong>, Xiaoguang Hu, Zhanyi Liu, Jianfeng Li, Dengjun Ren, Zheng-Yu Niu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/s10590-008-9041-6">Pivot language approach for phrase-based statistical machine translation</a></em>, Machine Translation 2007
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P07-1108/">Pivot Language Approach for Phrase-Based Statistical Machine Translation</a></em>, ACL 2007
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="http://mt-archive.info/MTS-2007-Wu.pdf">Comparative study of word alignment heuristics and phrase-based SMT</a></em>, MT Summit 2007
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="http://www.toshiba.com.cn/download/other/papers/MTSummit07_EBMT.pdf">Log-Linear generation models for example-based machine translation</a></em>, MT Summit 2007
        <br>Zhanyi Liu, Haifeng Wang, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a>Using RBMT Systems to Produce Bilingual Corpus for SMT</a></em>, EMNLP-CoNLL 2007
        <br>Xiaoguang Hu, Haifeng Wang, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/s10590-006-9016-4">Example-based machine translation based on tree-string correspondence and statistical generation</a></em>, Machine translation 2006
        <br>Zhanyi Liu, Haifeng Wang, <strong><u>Hua Wu</u></strong><br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P06-2112/">Word Alignment for Languages with Scarce Resources Using Bilingual Corpora of Other Language Pairs</a></em>, ACL 2006
        <br>Haifeng Wang, <strong><u>Hua Wu</u></strong>, Zhanyi Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P06-2117/">Boosting Statistical Word Alignment Using Labeled and Unlabeled Data</a></em>, ACL 2006
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang, Zhanyi Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P05-1058/">Alignment Model Adaptation for Domain-Specific Word Alignment</a></em>, ACL 2005
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang, Zhanyi Liu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/11562214_41">Improving Statistical Word Alignment with Ensemble Methods</a></em>, IJCNLP 2005
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.semanticscholar.org/paper/Improving-Translation-Memory-with-Word-Alignment-Hua-Hai-feng/ce8b9aac2f9a11ebcbdb3a161b31c00ea23cb797?p2df">Improving Translation Memory with Word Alignment Information</a></em>,  MT SUMMIT 2005
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang, Zhanyi Liu, Kai Tang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.478.8769&rep=rep1&type=pdf">Boosting statistical word alignment</a></em>, MT SUMMIT 2005
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/P04-3002/">Improving Domain-Specific Word Alignment for Computer Assisted Translation</a></em>, ACL (Poster and Demonstration) 2004
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://doi.org/10.1007/978-3-540-30194-3_29">Improving Domain-Specific Word Alignment with a General Bilingual Corpus</a></em>, AMTA 2004
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href="https://www.aclweb.org/anthology/C04-1005.pdf">Improving Statistical Word Alignment with a Rule-Based Machine Translation System</a></em>, COLING 2004
        <br><strong><u>Hua Wu</u></strong>, Haifeng Wang<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href = "https://www.aclweb.org/anthology/P03-1016.pdf">Synonymous collocation extraction using translation information</a></em>, ACL 2003
        <br><strong><u>Hua Wu</u></strong>, Ming Zhou<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href = "https://www.aclweb.org/anthology/W03-1610.pdf">Optimizing synonym extraction using monolingual and bilingual resources</a></em>, Proceedings of the second international workshop on Paraphrasing 2003
        <br><strong><u>Hua Wu</u></strong>, Ming Zhou<br>
        <div id="clear"></div>
    </li>


    <li>
        <em><a href = "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.617.4221&rep=rep1&type=pdf">Speech Corpus Collection and Annotation</a></em>
        <br>Li Aijun, Chen Xiaoxia, Sun Guohua, <strong><u>Hua Wu</u></strong>, Yin Zhigang, Zu Yiqing<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href = "https://pdfs.semanticscholar.org/e5ce/dac0df5aff4511dc1ea0b513b20b0ad8f5d5.pdf">A Generation System for Chinese Texts</a></em>, Sixth International Conference on Spoken Language Processing 2002
        <br><strong><u>Hua Wu</u></strong>, Taiyi HUANG, Bo Xu<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href = "https://www.aclweb.org/anthology/C00-2174.pdf">Chinese generation in a spoken dialogue translation system</a></em>, COLING 2002
        <br><strong><u>Hua Wu</u></strong>, Taiyi HUANG, Chengqing Zong<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href = "http://www.nlpr.ia.ac.cn/cip/ZongPublications/1999-%20Zong-NLPRS.pdf">Analysis on Characteristics of Chinese Spoken Language</a></em>, Proc. of 5th Natural Language Processing Pacific Rim Symposium 1999
        <br>Chengqing Zong, <strong><u>Hua Wu</u></strong>, Taiyi HUANG, Bo XU<br>
        <div id="clear"></div>
    </li>

    <li>
        <em><a href = "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.545.7341&rep=rep1&type=pdf">An INTERLINGUA FOR DIALOGUE TRANSLATION</a></em>
        <br><strong><u>Hua Wu</u></strong>, Taiyi HUANG, Bo XU<br>
        <div id="clear"></div>
    </li>
    <br>


    
    <!-- <h2>Workshops</h2> -->
</ul>
</div>


<!-- <a name="pat"></a> -->
<!-- <div id="line"></div> -->

<!-- <div id="Patents">
    <h1>Patents</h1>

    <ul>
	<li>
        <em> <a href="https://www.google.com/patents/US20130097178">Question and Answer Forum Techniques</a></em>, 
        <a>US Patent 2013</a> 
        <br>
		Young-In Song, <strong><u>Jing Liu</u></strong>, Chin-Yew Lin and Tetsuya Sakai <br>       
		<a href="https://www.google.com/patents/US20130097178"> US20130097178 A1</a> 
        <br>
        <div id="clear"></div>
    </li>
	<br>
	<li>
        <em> <a href="http://www.google.com/patents/US20130262453">Estimating Thread Participant Expertise Using A Competition-Based Model</a></em>, 
        <a>US Patent 2013</a> 
        <br>
		Chin-Yew Lin, Young-In Song and <strong><u>Jing Liu</u></strong> <br>       
		<a href="http://www.google.com/patents/US20130262453"> US20130262453 A1</a> 
        <br>
        <div id="clear"></div>
    </li>
    </ul>
    <div id="clear"></div>
</div> -->



<!-- <a name="links"></a>
<div id="line"></div> -->

<!-- <div id="Friends">
    <h1>Friends</h1>

<ul>
<li><a href="https://sites.google.com/site/quanwang1012/">Quan Wang</a>&nbsp;(Chinese Academy of Sciences @ Beijing)&nbsp;&nbsp;&nbsp;</li>
<li><a href="http://net.pku.edu.cn/~zhaoxin/">Xin Zhao</a>&nbsp;(Renmin University @ Beijing)&nbsp;&nbsp;&nbsp;</li>
<li><a href="https://sites.google.com/site/zhfan555/">Fan Zhang</a>&nbsp;(Google @ Mountain View)&nbsp;&nbsp;&nbsp;</li>
<li><a href="https://sites.google.com/site/zhenliao2012/">Zhen Liao</a>&nbsp;(Facebook @ Menlo Park)&nbsp;&nbsp;&nbsp;</li>
<li><a href="http://nlp.suda.edu.cn/~zoubowei/">Bowei Zou</a>&nbsp;(Soochow University @ Suzhou)&nbsp;&nbsp;&nbsp;</li>
<li><a href="http://ir.hit.edu.cn/~rjfu/">Ruiji Fu</a>&nbsp;(iFLYTEK @ Beijing)&nbsp;&nbsp;&nbsp;</li>
</ul>
	<br>
    <div id="clear"></div>
</div>-->

</div> 

<div id="design">Last updated <a>Dec 20 2020</a> (This template was originally designed by <a href="http://www.cs.cmu.edu/~muli/">Mu Li</a>.)</div>

</body>
</html>
